## ResNet
#### [https://arxiv.org/pdf/1512.03385v1.pdf](https://arxiv.org/pdf/1512.03385v1.pdf)
* 论文中提到，近几年的研究发现网络的深度是使网络性能更优化的一个关键因素，但是随着网络深度的加深，梯度消失&爆炸问题十分明显，网络甚至出现了退化。在论文中通过一个20层和一个56层的普通网络进行了对比，发现56层网络的性能远低于20层网络，如图1所示。</br>
![](http://or9rl648h.bkt.clouddn.com/17-6-9/15370595.jpg)
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
* 而在ResNet的这篇论文中，通过引入一个深度残差学习框架，解决了这个退化问题。它不期望每一层能直接吻合一个映射，而是明确的让这些层去吻合残差映射。形式上看，就是用H(X)来表示最优解映射，但我们让堆叠的非线性层去拟合另一个映射F（X）:=H(X) - X, 此时原最优解映射H（X）就可以改写成F(X)+X，我们假设残差映射跟原映射相比更容易被优化。极端情况下，如果一个映射是可优化的，那也会很容易将残差推至0，把残差推至0和把此映射逼近另一个非线性层相比要容易的多。F(X)+X的公式可以通过在前馈网络中做一个“快捷连接”来实现 ，快捷连接跳过一个或多个层。在我们的用例中，快捷连接简单的执行自身映射，它们的输出被添加到叠加层的输出中。自身快捷连接既不会添加额外的参数也不会增加计算复杂度。整个网络依然可以用SGD+反向传播来做端到端的训练。
* 在实际情况下，恒等映射不太可能是最佳的，但是我们的重新设计可能有助于预先解决问题。如果最优函数比零映射更接近于恒等映射，则求解器参考恒等映射找到扰动应该更容易，而不是将该函数作为新的函数学习。**why？感觉就是try出来的**</br>
$$ y =F(x,{W_i})+x  $$
如果x与F的维度不一样：
$$ y =F(x,{W_i})+W_sx  $$
F可以试验2、3层</br>
下图表示在残差网络在参数上并没有额外增加，比VGG-net参数减少
<div align=center>
![](http://or9rl648h.bkt.clouddn.com/17-6-9/4588677.jpg)
</div>
</br>
</br>**具体结构如下：**</br>
<div align=center>
![](http://or9rl648h.bkt.clouddn.com/17-6-9/72646839.jpg)
</div>


